{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import transformers as tfs\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertOnlyMLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertMLM1(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BertMLM1, self).__init__()\n",
    "        # 加载预训练好的BERT，使用其中的参数来初始化MLM\n",
    "        self.bert = BertForMaskedLM.from_pretrained(bert_path)\n",
    "\n",
    "    '''\n",
    "    这里用下载好的预训练模型来初始化我们的MLM模型\n",
    "    BertMLM分为三部分：1.embedding层 2.12个encoder层 3.线性分类层cls\n",
    "    一般来说，不同的任务，需要去更改第三个部分。MLM本质上是输出一堆word的probability，可以看做是分类任务\n",
    "    inputs_ids等三个参数的维度都是[batch_size, max_len]。它们是通过transformers库中的BertTokenizer模块来处理输入的句子得出的\n",
    "    '''\n",
    "\n",
    "    def forward(self, input_ids, input_tyi, input_attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, input_tyi=input_tyi, attention_mask=input_attention_mask)\n",
    "        print(input_ids)\n",
    "        print(input_tyi)\n",
    "        print(input_attention_mask)\n",
    "        return out['logits']\n",
    "\n",
    "\n",
    "class BertMLM2(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BertMLM2, self).__init__()\n",
    "        # 加载超参数\n",
    "        config = tfs.AutoConfig.from_pretrained(bert_path)\n",
    "        '''\n",
    "        注意：这里的bert_path是bert模型所在的文件夹，而不是直接指向bert模型本身，其中包含了config.json\n",
    "        是不是预先规定了，遇到了config.json就把它当做配置文件？\n",
    "        '''\n",
    "        self.embedding = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "    def forward(self, input_ids, input_tyi, input_attention_mask):\n",
    "        embeddings = self.embedding(input_ids=input_ids, token_type_ids=input_tyi)\n",
    "        '''\n",
    "        原始输入的input_attention_mask的shape是[batch_size, 1, 1, max_len]\n",
    "        但是bert encoder要求维度为[batch_size, max_len]\n",
    "        所以需要升维两次\n",
    "        '''\n",
    "        input_attention_mask = torch.squeeze(input_attention_mask, dim=1)\n",
    "        input_attention_mask = torch.squeeze(input_attention_mask, dim=1)\n",
    "\n",
    "        encoder_out = self.encoder(hidden_states=embeddings, attention_mask=input_attention_mask)\n",
    "        out_bert = encoder_out[0]\n",
    "        cls_out = self.cls(out_bert)\n",
    "        # cls_out的shape为[batch_size. max_len, vocab_size]，即batch_size个句子中，每个句子都有max_len个字，每个字映射到词表vocab_size个字的时候，其概率的大小"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_path = 'bert-based-chinese'\n",
    "bert_model = BertForMaskedLM.from_pretrained(bert_path)\n",
    "print(\"下面是bert模型可供调用查看的参数名\")\n",
    "print(bert_model.state_dict().keys())\n",
    "mlm_model = BertMLM2(bert_path)\n",
    "print(\"下面是BertEmbeddings模型可供调用查看的参数名\")\n",
    "print(mlm_model.embedding.state_dict().keys())\n",
    "print(\"下面是BertEncoder模型可供调用查看的参数名\")\n",
    "print(mlm_model.encoder.state_dict().keys())\n",
    "print(\"下面是BertOnlyMLMHead模型可供调用查看的参数名\")\n",
    "print(mlm_model.cls.state_dict().keys())\n",
    "# 可以看到bert模型对应其余三个模型的参数名均有改变，我们需要讲其参数名称改变一下，再统一赋值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# 将前j个前缀干掉\n",
    "def rm_forward(key_str, j):\n",
    "    key_list = key_str.split('.')\n",
    "    key_list = key_list[j:]\n",
    "    return '.'.join(key_list)\n",
    "\n",
    "class BertMLM(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BertMLM, self).__init__()\n",
    "        config = tfs.AutoConfig.from_pretrained(bert_path)\n",
    "        self.embedding = BertEmbeddings(config).to('cuda')\n",
    "        self.encoder = BertEncoder(config).to('cuda')\n",
    "        self.cls = BertOnlyMLMHead(config).to('cuda')\n",
    "        self.bert = BertForMaskedLM.from_pretrained(bert_path).to('cuda')\n",
    "\n",
    "        i = 0\n",
    "        emd_i = 6 # embedding层参数是6个\n",
    "        enc_i = 198 # encoder层参数是192个\n",
    "        cls_i = 205 # cls层参数是7个\n",
    "        for key, value in self.bert.state_dict().items():\n",
    "            if i < emd_i:\n",
    "                key = rm_forward(key, 2)\n",
    "                self.embedding.state_dict()[key].copy_(value)\n",
    "            elif i < enc_i:\n",
    "                key = rm_forward(key, 2)\n",
    "                self.encoder.state_dict()[key].copy_(value)\n",
    "            else:\n",
    "                key = rm_forward(key, 1)\n",
    "                self.cls.state_dict()[key].copy_(value)\n",
    "            i += 1\n",
    "\n",
    "    def forward(self, input_ids, input_tyi, input_attention_mask, mask_ids):\n",
    "        embeddings = self.embedding(input_ids=input_ids, token_type_ids=input_tyi)\n",
    "        '''\n",
    "        原始输入的input_attention_mask的shape是[batch_size, 1, 1, max_len]\n",
    "        但是bert encoder要求维度为[batch_size, max_len]\n",
    "        所以需要升维两次\n",
    "        '''\n",
    "        # input_attention_mask = torch.squeeze(input_attention_mask, dim=1)\n",
    "        # input_attention_mask = torch.squeeze(input_attention_mask, dim=1)\n",
    "\n",
    "        encoder_out = self.encoder(hidden_states=embeddings, attention_mask=input_attention_mask)\n",
    "        out_bert = encoder_out[0]\n",
    "        cls_out = self.cls(out_bert)\n",
    "        return cls_out[0][mask_ids]\n",
    "        # cls_out的shape为[batch_size. max_len, vocab_size]，即batch_size个句子中，每个句子都有max_len个字，每个字映射到词表vocab_size个字的时候，其概率的大小"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '中', '国', '的', '首', '都', '是', '[MASK]', '[MASK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-based-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1525 6443 1567  784 1408  862 2124  720 6306 1658]\n",
      " [8043  511 8013  136 1408 8024 1450 8038 8106  720]]\n",
      "[['哪', '谁', '啥', '什', '吗', '何', '它', '么', '誰', '嘛'], ['？', '。', '！', '?', '吗', '，', '呢', '：', '...', '么']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "text = \"[CLS]中国的首都是北京[SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text) # tokenize函数似乎就是把一个句子拆成一个字的list\n",
    "word_to_mask_ids = [7, 8] # 指定哪个字需要被mask\n",
    "for id in word_to_mask_ids:\n",
    "    tokenized_text[id] = '[MASK]'\n",
    "print(tokenized_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # 根据tokenizer.json，将字转换为索引\n",
    "segment_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "attention_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "token_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "segment_tensor = torch.tensor([segment_ids]).to('cuda')\n",
    "attention_mask = torch.tensor([attention_mask]).to('cuda')\n",
    "\n",
    "bert_mlm = BertMLM(bert_path)\n",
    "cls_out = bert_mlm.forward(token_tensor, segment_tensor, attention_mask, word_to_mask_ids)\n",
    "prediction_ids = torch.topk(cls_out, 10, sorted=True).indices.cpu().detach().numpy()\n",
    "prediction = []\n",
    "print(prediction_ids)\n",
    "for prediction_id in prediction_ids:\n",
    "    prediction.append(tokenizer.convert_ids_to_tokens(prediction_id))\n",
    "print(prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
