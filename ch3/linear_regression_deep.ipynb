{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data # 和normal不同的地方\n",
    "from d2l import torch as d2l\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.7869, -0.1846]), tensor([6.4129]))\n"
     ]
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    # 构造一个pytorch迭代器\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    '''\n",
    "    dataset里面有两个element，一个是X，一个是y\n",
    "    X的shape为(1000, true_w.shape[0])，每一行是一个example，总共1000行\n",
    "    y的shape为(1000, 1)，每一行是一个example，总共1000行\n",
    "    下面的dataloader主要是做了个顺序打乱工作，顺便根据batch_size返回对应数目的mini-batch\n",
    "    '''\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train, generator=torch.Generator(device='cuda')) # 新版pytorch记得在dataloader后指定generator的device\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "'''\n",
    "nn.Linear(m, n)定义了一个线性回归模型，第一层有m个unit，第二层有n个unit\n",
    "nn.Sequential里面的参数是把一堆model按顺序叠在一起\n",
    "'''\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "net[0].weight.data.normal_(0, 0.01) # 为线性模型的weight和bias初始化参数\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch @1 loss : 9.277692879550159e-05\n",
      "epoch @2 loss : 9.209680138155818e-05\n",
      "epoch @3 loss : 9.198694169754162e-05\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in data_iter:\n",
    "        '''\n",
    "        1.拿出一个小批量的数据（注意这里的X是矩阵，y是向量，计算loss\n",
    "        2.清零梯度\n",
    "        3.根据loss反向传播一次（这一步之后就求出了导数）\n",
    "        4.迭代一次\n",
    "        5.重复上面步骤直到拿完所有数据\n",
    "        '''\n",
    "        l = loss(net(X),  y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch @{} loss : {}'.format(epoch + 1, float(l)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
